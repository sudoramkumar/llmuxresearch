# llmuxresearch

The goal: explore whether large language models can simulate user feedback as realistically as human respondents.

ğŸ‘¥ Simulated Personas
Using prompt-based generation, I created ~25 synthetic personas across varied age groups, occupations, demographics, and motivations â€” each representing a different customer archetype for a q-commerce scenario.

ğŸ–¼ï¸ Experience Evaluation
I captured the full IM HP webpage and used a Vision API to have each persona describe â€” in 2â€“3 sentences â€” their first impression, purchase intent, and reasoning.

ğŸ“Š Quantifying Insights
Each response was converted into a Likert-style rating by mapping feedback to anchor statements using cosine similarity. This approach produced more diverse and realistic results than simply asking an LLM to â€œrateâ€ a homepage â€” which often leads to uniformly positive outputs.

ğŸ’¡ What I Found
The results mirrored human-like patterns (KS â‰ˆ 0.88 as noted in the research), with richer â€œwhyâ€ explanations behind each intent. It opens possibilities for creating AI-powered Digital Twins or automated customer advisory panels, replacing traditional, slow UX surveys with faster, deeper, and more scalable feedback loops.

âš ï¸ Caveats
Careful anchor design is critical, and it performs best for familiar product domains where the model has prior exposure.

ğŸ“‚ The full persona feedback and scoring workflow â€” mostly automated with minimal manual touchpoints â€” is documented in my shared sheet.
